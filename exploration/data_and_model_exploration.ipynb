{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA  # for compactness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/alexanderdesouza/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexanderdesouza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexanderdesouza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's sentiment intensity analyzer is based on a rule-based model that implements the model described here  _Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max.columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = pd.read_csv('../data/abcnews_million_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185464</th>\n",
       "      <td>20050831</td>\n",
       "      <td>leslie tests positive for drug use police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657266</th>\n",
       "      <td>20111202</td>\n",
       "      <td>missing man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622744</th>\n",
       "      <td>20110624</td>\n",
       "      <td>push on for indigenous constitutional recognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                           headline\n",
       "185464  20050831          leslie tests positive for drug use police\n",
       "657266  20111202                                        missing man\n",
       "622744  20110624  push on for indigenous constitutional recognition"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One million documents presents an extremely large corpus, that for the sake of this demonstration will make training excessively long without a smarter way in which to build up the TF-IDF matrix. For the present context then we randomly sample 1,000 documents for the experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = raw_headlines.sample(1000)\n",
    "raw_headlines.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20130819</td>\n",
       "      <td>senex energy granted exploration licence for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>20140603</td>\n",
       "      <td>woman found dead at lithgow in suspicious circ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>20050814</td>\n",
       "      <td>principals seek review of report card system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                           headline\n",
       "23   20130819  senex energy granted exploration licence for c...\n",
       "500  20140603  woman found dead at lithgow in suspicious circ...\n",
       "216  20050814       principals seek review of report card system"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A semantic intensity analyzer is constructed using NLTK's SIA (a robust, but rule-based, lexicographic heuristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()  # initialize a nltk semantic intensity analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_headlines = []\n",
    "\n",
    "for headline in raw_headlines['headline'].values:\n",
    "    sia_scores = sia.polarity_scores(headline)\n",
    "    sia_scores['headline'] = headline\n",
    "    scored_headlines.append(sia_scores)\n",
    "    \n",
    "headlines = pd.DataFrame(scored_headlines)\n",
    "# headlines['date'] = raw_headlines['date']  # in application the date is irrelevant and thus omitted here from further consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>headline</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>six month reprieve for dubbo grandstand</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.4215</td>\n",
       "      <td>lolo jones admits to emotional roller coaster ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>binge drinking on peoples minds ama</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>the road ahead for the relationship between</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>-0.3612</td>\n",
       "      <td>man dead another questioned by police sunshine...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     compound                                           headline   neg    neu  \\\n",
       "33     0.0000            six month reprieve for dubbo grandstand  0.00  1.000   \n",
       "433    0.4215  lolo jones admits to emotional roller coaster ...  0.00  0.648   \n",
       "197    0.0000                binge drinking on peoples minds ama  0.00  1.000   \n",
       "649    0.0000        the road ahead for the relationship between  0.00  1.000   \n",
       "974   -0.3612  man dead another questioned by police sunshine...  0.41  0.360   \n",
       "\n",
       "       pos  \n",
       "33   0.000  \n",
       "433  0.352  \n",
       "197  0.000  \n",
       "649  0.000  \n",
       "974  0.230  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129615"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The memory footprint of this object is...\n",
    "sys.getsizeof(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly a single Mb, which is sufficiently small as to be maintainable in memory by the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compactness the set is divided into positively and negatively associated corpuses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pve_articles = headlines[headlines['compound'] >= 0.0]\n",
    "nve_articles = headlines[headlines['compound'] < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.4% articles have positive sentiment\n",
      "35.6% articles have negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Distribution of the articles\n",
    "print(\"{:.1f}% articles have positive sentiment\".format(len(pve_articles)/(len(pve_articles)+len(nve_articles))*100))\n",
    "print(\"{:.1f}% articles have negative sentiment\".format(len(nve_articles)/(len(pve_articles)+len(nve_articles))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total length is preserved\n",
    "len(pve_articles)+len(nve_articles)==len(raw_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct some regular expressions to be able to clean any text that is input into the system, and define a preprocessor method that lower cases the input text, \"cleans\" abbreviations, and removes general special characters, and strips dashes and underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_abbr = re.compile(r'(?:^|\\s)((?:\\w(\\.\\s|\\s|\\.))(?:\\w\\2)+)', re.UNICODE)\n",
    "re_abbr_separator = re.compile(r'(\\s|\\.)', re.UNICODE)\n",
    "re_numword = re.compile(r'(\\s\\d*\\s)|\\s\\d*\\.\\d*\\s', re.UNICODE)\n",
    "re_specialchar_removal = re.compile(r'(!|@|#|&|\\(|\\)|\\+|=|\\{|\\}|\\[|\\]|:|;|\\\"|\\'|,|\\.$|\\?)', re.UNICODE)\n",
    "re_specialchar_numsymbremoval = re.compile(r'(\\$|%)', re.UNICODE)\n",
    "re_dash_removal = re.compile(r'-|_', re.UNICODE)\n",
    "\n",
    "\n",
    "def abbreviations_to_words(text):\n",
    "    \"\"\"\n",
    "    Converts all abbreviations found in the input string to a single word format.\n",
    "    \"\"\"\n",
    "    text += \" \"\n",
    "    all_abbreviations = [x[0] for x in re_abbr.findall(text + \" \")]\n",
    "    for abbreviation in all_abbreviations:\n",
    "        new_form = re_abbr_separator.sub('', abbreviation)\n",
    "        text = text.replace(abbreviation, new_form)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"\n",
    "    Applies the following preprocessing steps to any input text:\n",
    "        - lowercases all text\n",
    "        - maps abbreviations to same format (e.g., A.D., A. D., A D to AD)\n",
    "        - removes general special characters (e.g., an '!' or an '&' symbol)\n",
    "        - splits words that contains dashes or underscores\n",
    "        - strips the any newline characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = abbreviations_to_words(text)\n",
    "    text = re_specialchar_removal.sub('', text)\n",
    "    text = re_dash_removal.sub(' ', text)\n",
    "    text = re_specialchar_numsymbremoval.sub('', text) # depending on intent, this should be optional\n",
    "    text = re_numword.sub(' numword ', text)           # and this one\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def score_text(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scored_text = []\n",
    "    sia_score = sia.polarity_scores(text)\n",
    "    sia_score['headline'] = text\n",
    "    scored_text.append(sia_score)\n",
    "    return pd.DataFrame(scored_text)\n",
    "\n",
    "\n",
    "def add_article(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    global pve_articles\n",
    "    global nve_articles\n",
    "    text = preprocessor(text)\n",
    "    scored_text = score_text(text)\n",
    "    if scored_text['compound'].values[0] >= 0:\n",
    "        pve_articles = pd.concat([pve_articles, scored_text]).reset_index(drop=True)\n",
    "        print(\"\\\"{}\\\" added to the pve article set with score {}.\".format(scored_text['headline'].values[0], scored_text['compound'].values[0]))\n",
    "        return 1\n",
    "    else:\n",
    "        nve_articles = pd.concat([nve_articles, scored_text]).reset_index(drop=True)\n",
    "        print(\"\\\"{}\\\" added to the nve article set with score {}.\".format(scored_text['headline'].values[0], scored_text['compound'].values[0]))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Googled around for some headlines from CBC.ca/news, NPR.org, Bloomberg.com, and the MIT Technology Review (https://www.technologyreview.com/) to test the functionality of this preprocessing; the resulting examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_tests = [\"Hi! My name is Alexander.\",  # not actually a news item :P\n",
    "                  \"\\'Storm of a lifetime\\': 1.7 million ordered to flee approaching fury of Florence\",\n",
    "                  \"Trump Administration Transferred $9.8-Million From F.E.M.A. To I.C.E.\",\n",
    "                  \"A $100 Million Haircut for the Buyout Crowd\",\n",
    "                  \"Crypto Plunges 80%! Now Worse Than the Dot-Com Crash!\",\n",
    "                  \"How Bank Workers Emerged From the Crash $12.5 Billion Richer\",\n",
    "                  \"H.N.A.'s Debt Declines for First Time, Shrinking by $8.3-Billion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi my name is alexander',\n",
       " 'storm of a lifetime numword million ordered to flee approaching fury of florence',\n",
       " 'trump administration transferred numword million from fema to ice',\n",
       " 'a numword million haircut for the buyout crowd',\n",
       " 'crypto plunges numword now worse than the dot com crash',\n",
       " 'how bank workers emerged from the crash numword billion richer',\n",
       " 'hnas debt declines for first time shrinking by numword billion']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_processed_tests = []\n",
    "for hl in headline_tests:\n",
    "    headline_processed_tests.append(preprocessor(hl))\n",
    "\n",
    "headline_processed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the articles, performing the preprocessing as an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hi my name is alexander\" added to the pve article set with score 0.0.\n",
      "\"storm of a lifetime numword million ordered to flee approaching fury of florence\" added to the nve article set with score -0.5719.\n",
      "\"trump administration transferred numword million from fema to ice\" added to the pve article set with score 0.0.\n",
      "\"a numword million haircut for the buyout crowd\" added to the pve article set with score 0.0.\n",
      "\"crypto plunges numword now worse than the dot com crash\" added to the nve article set with score -0.7003.\n",
      "\"how bank workers emerged from the crash numword billion richer\" added to the pve article set with score 0.1779.\n",
      "\"hnas debt declines for first time shrinking by numword billion\" added to the nve article set with score -0.3612.\n"
     ]
    }
   ],
   "source": [
    "for hl in headline_tests:\n",
    "    add_article(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a new headline is introduced that is roughly equivalent to one of the above to act as a comparative reference in evaluting the efficacy of what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big bank employees came out wealthier from the great recession'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_headline = preprocessor(\"Big Bank Employees Came Out Wealthier from the Great Recession!\")\n",
    "new_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match different, but analogous, sentences the body text should be simplified. One way to accomplish this is to remove stopwords, stem, and lemmatize all the sentences to be matched. Here, a tokenizer and lemmatizer are applied to the text. Text objects can then be compared by calculating and evaluating the cosine distance between the resultant document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initial attempt utilized Porter Stemming instead of Lemmatization\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  # porter stemming, rule-based word reduction\n",
    "    tokens = nltk.word_tokenize(text)  # generate word tokens\n",
    "    return [i for i in [stemmer.stem(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above method is recomposed as a class that can be passed directly to the TF-IDF vectorizer that follows;\n",
    "# lemmatization is selected and applied in place of the Porter stemming\n",
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'bank',\n",
       " 'employe',\n",
       " 'came',\n",
       " 'out',\n",
       " 'wealthier',\n",
       " 'from',\n",
       " 'the',\n",
       " 'great',\n",
       " 'recess']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visual inspection of the output from the TokenLemmatizer() on the `new_headline`\n",
    "tokenize_and_stem(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is used to construct the vectorized forms of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance while varying `ngram_range` between one and three resulted in significantly decreased performance. This is likely due to the fact that we're specifically analyzing analogue sentences, where not only are the words between two documents potentially in a different order, but synonyms for words may effect the groupings as well. In such a case the construction of n-grams will artificially increase the distance between associated word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of `max_df` and `min_df` below made intuitively with several manual iterations used to optimize performance (default values are [1.0, 1.0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)  # hence, no n-grams here\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(headline_processed_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>administration</th>\n",
       "      <th>alexander</th>\n",
       "      <th>approaching</th>\n",
       "      <th>bank</th>\n",
       "      <th>billion</th>\n",
       "      <th>buyout</th>\n",
       "      <th>com</th>\n",
       "      <th>crash</th>\n",
       "      <th>crowd</th>\n",
       "      <th>crypto</th>\n",
       "      <th>debt</th>\n",
       "      <th>decline</th>\n",
       "      <th>dot</th>\n",
       "      <th>emerged</th>\n",
       "      <th>fema</th>\n",
       "      <th>flee</th>\n",
       "      <th>florence</th>\n",
       "      <th>fury</th>\n",
       "      <th>haircut</th>\n",
       "      <th>hnas</th>\n",
       "      <th>ice</th>\n",
       "      <th>lifetime</th>\n",
       "      <th>million</th>\n",
       "      <th>ordered</th>\n",
       "      <th>plunge</th>\n",
       "      <th>richer</th>\n",
       "      <th>shrinking</th>\n",
       "      <th>storm</th>\n",
       "      <th>time</th>\n",
       "      <th>transferred</th>\n",
       "      <th>trump</th>\n",
       "      <th>worker</th>\n",
       "      <th>worse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.259024</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.348019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.357939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   administration  alexander  approaching      bank   billion    buyout  \\\n",
       "0        0.000000        1.0     0.000000  0.000000  0.000000  0.000000   \n",
       "1        0.000000        0.0     0.365065  0.000000  0.000000  0.000000   \n",
       "2        0.426268        0.0     0.000000  0.000000  0.000000  0.000000   \n",
       "3        0.000000        0.0     0.000000  0.000000  0.000000  0.534261   \n",
       "4        0.000000        0.0     0.000000  0.000000  0.000000  0.000000   \n",
       "5        0.000000        0.0     0.000000  0.431207  0.357939  0.000000   \n",
       "6        0.000000        0.0     0.000000  0.000000  0.348019  0.000000   \n",
       "\n",
       "        com     crash     crowd    crypto      debt   decline       dot  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.534261  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.419257  0.348019  0.000000  0.419257  0.000000  0.000000  0.419257   \n",
       "5  0.000000  0.357939  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.419257  0.419257  0.000000   \n",
       "\n",
       "    emerged      fema      flee  florence      fury   haircut      hnas  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.365065  0.365065  0.365065  0.000000  0.000000   \n",
       "2  0.000000  0.426268  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.534261  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.431207  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.419257   \n",
       "\n",
       "        ice  lifetime   million   ordered    plunge    richer  shrinking  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "1  0.000000  0.365065  0.259024  0.365065  0.000000  0.000000   0.000000   \n",
       "2  0.426268  0.000000  0.302450  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.379074  0.000000  0.000000  0.000000   0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.419257  0.000000   0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.431207   0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.419257   \n",
       "\n",
       "      storm      time  transferred     trump    worker     worse  \n",
       "0  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.365065  0.000000     0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000     0.426268  0.426268  0.000000  0.000000  \n",
       "3  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000     0.000000  0.000000  0.000000  0.419257  \n",
       "5  0.000000  0.000000     0.000000  0.000000  0.431207  0.000000  \n",
       "6  0.000000  0.419257     0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's take a look at what the train_tfidf matrix looks like, as a dataframe\n",
    "pd.DataFrame(train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting a new sentence against this corpus, the sentence's similarity is evaluated as the cosine similarity between the new sentence and the existing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "response = tfidf_vectorizer.transform([new_headline])\n",
    "print(response)  # CSR result, feature(s) enumerated and significance, matching entirely due to the presence of the word 'bank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our matricies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x33 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x33 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.43120736, 0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = linear_kernel(response, train_tfidf).flatten()\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.4312073587067964)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retreive a tuple of the maximally similar result\n",
    "np.argmax(similarity), max(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we rearrange this code for future implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and reprint our corpus and reframe the `new_headline` in the form of a query\n",
    "pve_corpus = pve_articles['headline'].values\n",
    "nve_corpus = nve_articles['headline'].values\n",
    "sample_query = \"did bank employees come out wealthier from the recession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]\n",
    "\n",
    "\n",
    "class TfIdfer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.0)\n",
    "        self.tfidf_corpus = None\n",
    "        self.vocab = None\n",
    "        \n",
    "    def train(corpus):\n",
    "        self.tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "        self.vocab = self.tfidf_vectorizer.vocabulary_\n",
    "    \n",
    "    def transform(doc):\n",
    "        return self.tfidf_vectorizer.transform([doc])\n",
    "    \n",
    "    # def update():\n",
    "        # To do...\n",
    "        # self.tfidf_vectorizer = TfidfVectorizer(vocabulary=self.vocab,\n",
    "        #                                         stop_words='english',\n",
    "        #                                         tokenizer=TokenLemmatizer(),\n",
    "        #                                         max_df=0.5, min_df=0.0)\n",
    "\n",
    "\n",
    "def calculate_similarities(doc, corpus):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.0)\n",
    "    tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    doc_vector = tfidf_vectorizer.transform([doc])\n",
    "\n",
    "    return linear_kernel(doc_vector, tfidf_corpus).flatten()\n",
    "\n",
    "\n",
    "def retreive_results(similarities, corpus):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # argmax_index = np.argmax(similarities)\n",
    "    argmax_indicies = [np.where(similarities!=0)[0][:]]\n",
    "    score = max(similarities)\n",
    "    return {\"index\": argmax_indicies,\n",
    "            \"item\": corpus[np.argmax(similarities)],\n",
    "            \"score\": score}\n",
    "\n",
    "\n",
    "def query_articles(query_statement):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # processed_query_statement = query_statement  # will first have to process the query_statement, but here it's assumed that's been done\n",
    "    processed_query_statement = preprocessor(query_statement)  # forgot to make this call\n",
    "    print(processed_query_statement)\n",
    "    \n",
    "    pve_similarities = calculate_similarities(processed_query_statement, pve_articles['headline'].values)\n",
    "    nve_similarities = calculate_similarities(processed_query_statement, nve_articles['headline'].values)\n",
    "    \n",
    "    results[\"positive\"] = retreive_results(pve_similarities, pve_articles['headline'].values)\n",
    "    results[\"negative\"] = retreive_results(nve_similarities, nve_articles['headline'].values)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized testing below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294    three in court over melbourne cocaine bust\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pve_articles['headline'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350    tasmania police seek information on two seriou...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nve_articles['headline'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is baddeley confident\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positive': {'index': [array([ 44, 230, 577])],\n",
       "  'item': 'watson confident tigers will release him',\n",
       "  'score': 0.504988628218755},\n",
       " 'negative': {'index': [array([], dtype=int64)],\n",
       "  'item': 'andrew michael burke guilty of joan ryther murder life in prison',\n",
       "  'score': 0.0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_articles(\"Is Baddeley confident?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper hunter winerys waste warning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positive': {'index': [array([396])],\n",
       "  'item': 'flash flooding in the hunter',\n",
       "  'score': 0.6097046913553162},\n",
       " 'negative': {'index': [array([ 89, 156, 187, 213, 223, 310, 314, 316])],\n",
       "  'item': 'warnings over broadband uptake',\n",
       "  'score': 0.29499644109645606}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_articles(\"upper hunter winery's waste warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
