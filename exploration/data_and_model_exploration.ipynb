{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk.corpus\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# import nltk.stem.snowball\n",
    "# from nltk.corpus import wordnet\n",
    "# import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's sentiment intensity analyzer is based on a rule-based model that implements the model described here  _Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max.columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = pd.read_csv('../data/abcnews_million_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.columns = ['date', 'headline']  # rename columns 'cause the others were verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.sample(20)['headline'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()  # initialize a nltk semantic intensity analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_headlines = []\n",
    "\n",
    "for headline in raw_headlines.sample(20)['headline'].values:\n",
    "    sia_scores = sia.polarity_scores(headline)\n",
    "    sia_scores['headline'] = headline\n",
    "    scored_headlines.append(sia_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scored_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct some regular expressions to be able to clean any text that is input into the system, and define a preprocessor method that lower cases the input text, \"cleans\" abbreviations, and removes general special characters, and strips dashes and underscores.\n",
    "\n",
    "This is one step up from rudimentary; the trouble with regex is you're never done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_abbr = re.compile(r'(?:^|\\s)((?:\\w(\\.\\s|\\s|\\.))(?:\\w\\2)+)', re.UNICODE)\n",
    "re_abbr_separator = re.compile(r'(\\s|\\.)', re.UNICODE)\n",
    "re_numword = re.compile(r'(\\s\\d*\\s)|\\s\\d*\\.\\d*\\s', re.UNICODE)\n",
    "re_specialchar_removal = re.compile(r'(!|@|#|&|\\(|\\)|\\+|=|\\{|\\}|\\[|\\]|:|;|\\\"|\\'|,|\\.$|\\?)', re.UNICODE)\n",
    "re_specialchar_numsymbremoval = re.compile(r'(\\$|%)', re.UNICODE)\n",
    "re_dash_removal = re.compile(r'-|_', re.UNICODE)\n",
    "\n",
    "\n",
    "def abbreviations_to_words(text):\n",
    "    \"\"\"\n",
    "    Converts all abbreviations found in the input string to a single word format.\n",
    "    \"\"\"\n",
    "    text += \" \"\n",
    "    all_abbreviations = [x[0] for x in re_abbr.findall(text + \" \")]\n",
    "    for abbreviation in all_abbreviations:\n",
    "        new_form = re_abbr_separator.sub('', abbreviation)\n",
    "        text = text.replace(abbreviation, new_form)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"\n",
    "    Applies the following preprocessing steps to any input text:\n",
    "        - lowercases all text\n",
    "        - maps abbreviations to same format (e.g., A.D., A. D., A D to AD)\n",
    "        - removes general special characters (e.g., an '!' or an '&' symbol)\n",
    "        - splits words that contains dashes or underscores\n",
    "        - strips the any newline characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = abbreviations_to_words(text)\n",
    "    text = re_specialchar_removal.sub('', text)\n",
    "    text = re_dash_removal.sub(' ', text)\n",
    "    text = re_specialchar_numsymbremoval.sub('', text) # depending on intent, this should be optional\n",
    "    text = re_numword.sub(' numword ', text)           # and this one\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Googled around for some headlines from CBC.ca/news, NPR.org, Bloomberg.com, and the MIT Technology Review (https://www.technologyreview.com/) to test the functionality of this preprocessing; the resulting examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_tests = [\"Hi! My name is Alexander.\",  # not actually a news item :P\n",
    "                  \"\\'Storm of a lifetime\\': 1.7 million ordered to flee approaching fury of Florence\",\n",
    "                  \"Trump Administration Transferred $9.8-Million From F.E.M.A. To I.C.E.\",\n",
    "                  \"A $100 Million Haircut for the Buyout Crowd\",\n",
    "                  \"Crypto Plunges 80%! Now Worse Than the Dot-Com Crash!\",\n",
    "                  \"How Bank Workers Emerged From the Crash $12.5 Billion Richer\",\n",
    "                  \"H.N.A.'s Debt Declines for First Time, Shrinking by $8.3-Billion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_processed_tests = []\n",
    "for hl in headline_tests:\n",
    "    headline_processed_tests.append(preprocessor(hl))\n",
    "\n",
    "headline_processed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good enough!\n",
    "\n",
    "Introduce a new headline that is roughly equivalent to one of the above; this will be used for comparison in evaluting the effecacy of what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_headline = preprocessor(\"Big Bank Employees Came Out Even Wealthier from the Great Recession!\")\n",
    "new_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match different, but analogous, sentences the body text should be simplified. One way to accomplish this is to remove stopwords, stem, and lemmatize all the sentences to be matched. Here, a tokenizer and lemmatizer are applied to the text. Text objects can then be compared by calculating and evaluating the cosine distance between the resultant document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initial attempt utilized Porter Stemming instead of Lemmatization\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  # porter stemming, rule-based word reduction\n",
    "    tokens = nltk.word_tokenize(text)  # generate word tokens\n",
    "    return [i for i in [stemmer.stem(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above method is recomposed as a class that can be passed directly to the TF-IDF vectorizer that follows;\n",
    "# lemmatization is selected and applied in place of the Porter stemming\n",
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tokenize_and_stem(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is used for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance while varying `ngram_range` between one and three resulted in significantly decreased performance. This is likely due to the fact that we're specifically analyzing analogue sentences, where not only are the words between two documents in a different order, but synonyms may be in use as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of `max_df` and `min_df` below made intuitively with several manual iterations optimizing for performance (default values are [1.0, 1.0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=Lemmatizer(), max_df=0.5, min_df=0.01)  # hence, no n-grams here\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(headline_processed_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's take a look at what the train_tfidf matrix looks like, as a dataframe\n",
    "pd.DataFrame(train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting a new sentence against this corpus, the sentence's similarity is evaluated as the cosine similarity between the new sentence and the existing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tfidf_vectorizer.transform([new_headline])\n",
    "print(response)  # CSR result, feature(s) enumerated and significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our matricies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = linear_kernel(response, train_tfidf).flatten()\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive a tuple of the maximally similar result\n",
    "np.argmax(similarity), max(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we rearrange this code for future implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and reprint our corpus and reframe the `new_headline` in the form of a query\n",
    "corpus = headline_processed_tests\n",
    "sample_query = \"did bank employees come out even wealthier from the recession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]\n",
    "\n",
    "\n",
    "class TfIdfer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)\n",
    "        self.tfidf_corpus = None\n",
    "        self.vocab = None\n",
    "        \n",
    "    def train(corpus):\n",
    "        self.tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "        self.vocab = self.tfidf_vectorizer.vocabulary_\n",
    "    \n",
    "    def transform(doc):\n",
    "        return self.tfidf_vectorizer.transform([doc])\n",
    "    \n",
    "    # def update():\n",
    "        # To do...\n",
    "        # self.tfidf_vectorizer = TfidfVectorizer(vocabulary=self.vocab,\n",
    "        #                                         stop_words='english',\n",
    "        #                                         tokenizer=TokenLemmatizer(),\n",
    "        #                                         max_df=0.5, min_df=0.01)\n",
    "\n",
    "\n",
    "def calculate_similarities(query_statement):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Collect imports here for reference\n",
    "    # from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # from sklearn.metrics.pairwise import linear_kernel  # recall the linear_kernal method is optimized for vectorize dot-product\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)  # hence, no n-grams here\n",
    "    tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    query_vector = tfidf_vectorizer.transform([query_statement])\n",
    "\n",
    "    return linear_kernel(query_vector, tfidf_corpus).flatten()\n",
    "\n",
    "\n",
    "def retreive_results(similarities):\n",
    "    argmax_index = np.argmax(similarities)\n",
    "    score = max(similarities)\n",
    "    return argmax_index, corpus[argmax_index], score\n",
    "\n",
    "\n",
    "def query_articles(query_statement):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    processed_query_statement = query_statement  # will first have to process the query_statement, but here it's assumed that's been done\n",
    "    similarities = calculate_similarities(processed_query_statement)\n",
    "    results = retreive_results(similarities)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_articles(\"how bank workers emerged from the crash numword billion richer\")  # on the original doc itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_articles(sample_query)  # on the sample headline query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps split positive and negative sentiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
