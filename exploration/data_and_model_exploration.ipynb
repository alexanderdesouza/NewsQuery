{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA  # for compactness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/alexanderdesouza/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexanderdesouza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's sentiment intensity analyzer is based on a rule-based model that implements the model described here  _Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max.columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = pd.read_csv('../data/abcnews_million_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338577</th>\n",
       "      <td>20071019</td>\n",
       "      <td>victoria set solid 263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754472</th>\n",
       "      <td>20130111</td>\n",
       "      <td>tomic battles into sydney semis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361517</th>\n",
       "      <td>20080205</td>\n",
       "      <td>businesses spooked despite good times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                               headline\n",
       "338577  20071019                 victoria set solid 263\n",
       "754472  20130111        tomic battles into sydney semis\n",
       "361517  20080205  businesses spooked despite good times"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One million documents presents an extremely large corpus, that for the sake of this demonstration will make training excessively long without a smarter way in which to build up the TF-IDF matrix. For the present context then we randomly sample 1,000 documents for the experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = raw_headlines.sample(1000)\n",
    "raw_headlines.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>20160528</td>\n",
       "      <td>singapores military might on show in defence p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>20060412</td>\n",
       "      <td>rudd sees merit in china trade push</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>20140414</td>\n",
       "      <td>child care demand drops because of high jobles...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                           headline\n",
       "411  20160528  singapores military might on show in defence p...\n",
       "484  20060412                rudd sees merit in china trade push\n",
       "296  20140414  child care demand drops because of high jobles..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A semantic intensity analyzer is constructed using NLTK's SIA (a robust, but rule-based, lexicographic heuristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()  # initialize a nltk semantic intensity analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_headlines = []\n",
    "\n",
    "for headline in raw_headlines['headline'].values:\n",
    "    sia_scores = sia.polarity_scores(headline)\n",
    "    sia_scores['headline'] = headline\n",
    "    scored_headlines.append(sia_scores)\n",
    "    \n",
    "headlines = pd.DataFrame(scored_headlines)\n",
    "# headlines['date'] = raw_headlines['date']  # in application the date is irrelevant and thus omitted here from further consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>headline</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>union says jobs going under forestry policy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>new fish size limits take effect in qld</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>tomic made to work for quarter finals spot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.5106</td>\n",
       "      <td>three energy giants share gorgon gas project</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>jackson climate response made in the usa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     compound                                      headline  neg    neu    pos\n",
       "742    0.0000   union says jobs going under forestry policy  0.0  1.000  0.000\n",
       "847    0.0000       new fish size limits take effect in qld  0.0  1.000  0.000\n",
       "449    0.0000    tomic made to work for quarter finals spot  0.0  1.000  0.000\n",
       "924    0.5106  three energy giants share gorgon gas project  0.0  0.538  0.462\n",
       "356    0.0000      jackson climate response made in the usa  0.0  1.000  0.000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128631"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The memory footprint of this object is...\n",
    "sys.getsizeof(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly a single Mb, which is sufficiently small as to be maintainable in memory by the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compactness the set is divided into positively and negatively associated corpuses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pve_articles = headlines[headlines['compound'] >= 0.0]\n",
    "nve_articles = headlines[headlines['compound'] < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 or 64.7% articles have positive sentiment\n",
      "361 or 35.3% articles have negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Distribution of the articles\n",
    "print(\"{:.1f}% articles have positive sentiment\".format(len(pve_articles)/(len(pve_articles)+len(nve_articles))*100))\n",
    "print(\"{:.1f}% articles have negative sentiment\".format(len(nve_articles)/(len(pve_articles)+len(nve_articles))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total length is preserved\n",
    "len(pve_articles)+len(nve_articles)==len(raw_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct some regular expressions to be able to clean any text that is input into the system, and define a preprocessor method that lower cases the input text, \"cleans\" abbreviations, and removes general special characters, and strips dashes and underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_abbr = re.compile(r'(?:^|\\s)((?:\\w(\\.\\s|\\s|\\.))(?:\\w\\2)+)', re.UNICODE)\n",
    "re_abbr_separator = re.compile(r'(\\s|\\.)', re.UNICODE)\n",
    "re_numword = re.compile(r'(\\s\\d*\\s)|\\s\\d*\\.\\d*\\s', re.UNICODE)\n",
    "re_specialchar_removal = re.compile(r'(!|@|#|&|\\(|\\)|\\+|=|\\{|\\}|\\[|\\]|:|;|\\\"|\\'|,|\\.$|\\?)', re.UNICODE)\n",
    "re_specialchar_numsymbremoval = re.compile(r'(\\$|%)', re.UNICODE)\n",
    "re_dash_removal = re.compile(r'-|_', re.UNICODE)\n",
    "\n",
    "\n",
    "def abbreviations_to_words(text):\n",
    "    \"\"\"\n",
    "    Converts all abbreviations found in the input string to a single word format.\n",
    "    \"\"\"\n",
    "    text += \" \"\n",
    "    all_abbreviations = [x[0] for x in re_abbr.findall(text + \" \")]\n",
    "    for abbreviation in all_abbreviations:\n",
    "        new_form = re_abbr_separator.sub('', abbreviation)\n",
    "        text = text.replace(abbreviation, new_form)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"\n",
    "    Applies the following preprocessing steps to any input text:\n",
    "        - lowercases all text\n",
    "        - maps abbreviations to same format (e.g., A.D., A. D., A D to AD)\n",
    "        - removes general special characters (e.g., an '!' or an '&' symbol)\n",
    "        - splits words that contains dashes or underscores\n",
    "        - strips the any newline characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = abbreviations_to_words(text)\n",
    "    text = re_specialchar_removal.sub('', text)\n",
    "    text = re_dash_removal.sub(' ', text)\n",
    "    text = re_specialchar_numsymbremoval.sub('', text) # depending on intent, this should be optional\n",
    "    text = re_numword.sub(' numword ', text)           # and this one\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def score_text(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scored_text = []\n",
    "    sia_score = sia.polarity_scores(text)\n",
    "    sia_score['headline'] = text\n",
    "    scored_text.append(sia_score)\n",
    "    return pd.DataFrame(scored_text)\n",
    "\n",
    "\n",
    "def add_article(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    global pve_articles\n",
    "    global nve_articles\n",
    "    text = preprocessor(text)\n",
    "    scored_text = score_text(text)\n",
    "    if scored_text['compound'].values[0] >= 0:\n",
    "        pve_articles = pd.concat([pve_articles, scored_text]).reset_index(drop=True)\n",
    "        print(\"\\\"{}\\\" added to the pve article set with score {}.\".format(scored_text['headline'].values[0], scored_text['compound'].values[0]))\n",
    "        return 1\n",
    "    else:\n",
    "        nve_articles = pd.concat([nve_articles, scored_text]).reset_index(drop=True)\n",
    "        print(\"\\\"{}\\\" added to the nve article set with score {}.\".format(scored_text['headline'].values[0], scored_text['compound'].values[0]))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Googled around for some headlines from CBC.ca/news, NPR.org, Bloomberg.com, and the MIT Technology Review (https://www.technologyreview.com/) to test the functionality of this preprocessing; the resulting examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_tests = [\"Hi! My name is Alexander.\",  # not actually a news item :P\n",
    "                  \"\\'Storm of a lifetime\\': 1.7 million ordered to flee approaching fury of Florence\",\n",
    "                  \"Trump Administration Transferred $9.8-Million From F.E.M.A. To I.C.E.\",\n",
    "                  \"A $100 Million Haircut for the Buyout Crowd\",\n",
    "                  \"Crypto Plunges 80%! Now Worse Than the Dot-Com Crash!\",\n",
    "                  \"How Bank Workers Emerged From the Crash $12.5 Billion Richer\",\n",
    "                  \"H.N.A.'s Debt Declines for First Time, Shrinking by $8.3-Billion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi my name is alexander',\n",
       " 'storm of a lifetime numword million ordered to flee approaching fury of florence',\n",
       " 'trump administration transferred numword million from fema to ice',\n",
       " 'a numword million haircut for the buyout crowd',\n",
       " 'crypto plunges numword now worse than the dot com crash',\n",
       " 'how bank workers emerged from the crash numword billion richer',\n",
       " 'hnas debt declines for first time shrinking by numword billion']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_processed_tests = []\n",
    "for hl in headline_tests:\n",
    "    headline_processed_tests.append(preprocessor(hl))\n",
    "\n",
    "headline_processed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the articles, performing the preprocessing as an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hi my name is alexander\" added to the pve article set with score 0.0.\n",
      "\"storm of a lifetime numword million ordered to flee approaching fury of florence\" added to the nve article set with score -0.5719.\n",
      "\"trump administration transferred numword million from fema to ice\" added to the pve article set with score 0.0.\n",
      "\"a numword million haircut for the buyout crowd\" added to the pve article set with score 0.0.\n",
      "\"crypto plunges numword now worse than the dot com crash\" added to the nve article set with score -0.7003.\n",
      "\"how bank workers emerged from the crash numword billion richer\" added to the pve article set with score 0.1779.\n",
      "\"hnas debt declines for first time shrinking by numword billion\" added to the nve article set with score -0.3612.\n"
     ]
    }
   ],
   "source": [
    "for hl in headline_tests:\n",
    "    add_article(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a new headline is introduced that is roughly equivalent to one of the above to act as a comparative reference in evaluting the efficacy of what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big bank employees came out wealthier from the great recession'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_headline = preprocessor(\"Big Bank Employees Came Out Wealthier from the Great Recession!\")\n",
    "new_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match different, but analogous, sentences the body text should be simplified. One way to accomplish this is to remove stopwords, stem, and lemmatize all the sentences to be matched. Here, a tokenizer and lemmatizer are applied to the text. Text objects can then be compared by calculating and evaluating the cosine distance between the resultant document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initial attempt utilized Porter Stemming instead of Lemmatization\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  # porter stemming, rule-based word reduction\n",
    "    tokens = nltk.word_tokenize(text)  # generate word tokens\n",
    "    return [i for i in [stemmer.stem(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above method is recomposed as a class that can be passed directly to the TF-IDF vectorizer that follows;\n",
    "# lemmatization is selected and applied in place of the Porter stemming\n",
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'bank',\n",
       " 'employe',\n",
       " 'came',\n",
       " 'out',\n",
       " 'wealthier',\n",
       " 'from',\n",
       " 'the',\n",
       " 'great',\n",
       " 'recess']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visual inspection of the output from the TokenLemmatizer() on the `new_headline`\n",
    "tokenize_and_stem(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is used to construct the vectorized forms of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance while varying `ngram_range` between one and three resulted in significantly decreased performance. This is likely due to the fact that we're specifically analyzing analogue sentences, where not only are the words between two documents potentially in a different order, but synonyms for words may effect the groupings as well. In such a case the construction of n-grams will artificially increase the distance between associated word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of `max_df` and `min_df` below made intuitively with several manual iterations used to optimize performance (default values are [1.0, 1.0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)  # hence, no n-grams here\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(headline_processed_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>administration</th>\n",
       "      <th>alexander</th>\n",
       "      <th>approaching</th>\n",
       "      <th>bank</th>\n",
       "      <th>billion</th>\n",
       "      <th>buyout</th>\n",
       "      <th>com</th>\n",
       "      <th>crash</th>\n",
       "      <th>crowd</th>\n",
       "      <th>crypto</th>\n",
       "      <th>debt</th>\n",
       "      <th>decline</th>\n",
       "      <th>dot</th>\n",
       "      <th>emerged</th>\n",
       "      <th>fema</th>\n",
       "      <th>flee</th>\n",
       "      <th>florence</th>\n",
       "      <th>fury</th>\n",
       "      <th>haircut</th>\n",
       "      <th>hnas</th>\n",
       "      <th>ice</th>\n",
       "      <th>lifetime</th>\n",
       "      <th>million</th>\n",
       "      <th>ordered</th>\n",
       "      <th>plunge</th>\n",
       "      <th>richer</th>\n",
       "      <th>shrinking</th>\n",
       "      <th>storm</th>\n",
       "      <th>time</th>\n",
       "      <th>transferred</th>\n",
       "      <th>trump</th>\n",
       "      <th>worker</th>\n",
       "      <th>worse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.259024</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.426268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.348019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.357939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431207</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   administration  alexander  approaching      bank   billion    buyout  \\\n",
       "0        0.000000        1.0     0.000000  0.000000  0.000000  0.000000   \n",
       "1        0.000000        0.0     0.365065  0.000000  0.000000  0.000000   \n",
       "2        0.426268        0.0     0.000000  0.000000  0.000000  0.000000   \n",
       "3        0.000000        0.0     0.000000  0.000000  0.000000  0.534261   \n",
       "4        0.000000        0.0     0.000000  0.000000  0.000000  0.000000   \n",
       "5        0.000000        0.0     0.000000  0.431207  0.357939  0.000000   \n",
       "6        0.000000        0.0     0.000000  0.000000  0.348019  0.000000   \n",
       "\n",
       "        com     crash     crowd    crypto      debt   decline       dot  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.534261  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.419257  0.348019  0.000000  0.419257  0.000000  0.000000  0.419257   \n",
       "5  0.000000  0.357939  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.419257  0.419257  0.000000   \n",
       "\n",
       "    emerged      fema      flee  florence      fury   haircut      hnas  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.365065  0.365065  0.365065  0.000000  0.000000   \n",
       "2  0.000000  0.426268  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.534261  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.431207  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.419257   \n",
       "\n",
       "        ice  lifetime   million   ordered    plunge    richer  shrinking  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "1  0.000000  0.365065  0.259024  0.365065  0.000000  0.000000   0.000000   \n",
       "2  0.426268  0.000000  0.302450  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.379074  0.000000  0.000000  0.000000   0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.419257  0.000000   0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.431207   0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.419257   \n",
       "\n",
       "      storm      time  transferred     trump    worker     worse  \n",
       "0  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.365065  0.000000     0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000     0.426268  0.426268  0.000000  0.000000  \n",
       "3  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000     0.000000  0.000000  0.000000  0.419257  \n",
       "5  0.000000  0.000000     0.000000  0.000000  0.431207  0.000000  \n",
       "6  0.000000  0.419257     0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's take a look at what the train_tfidf matrix looks like, as a dataframe\n",
    "pd.DataFrame(train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting a new sentence against this corpus, the sentence's similarity is evaluated as the cosine similarity between the new sentence and the existing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "response = tfidf_vectorizer.transform([new_headline])\n",
    "print(response)  # CSR result, feature(s) enumerated and significance, matching entirely due to the presence of the word 'bank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our matricies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x33 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x33 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.43120736, 0.        ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = linear_kernel(response, train_tfidf).flatten()\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.4312073587067964)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retreive a tuple of the maximally similar result\n",
    "np.argmax(similarity), max(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we rearrange this code for future implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and reprint our corpus and reframe the `new_headline` in the form of a query\n",
    "pve_corpus = pve_articles['headline'].values\n",
    "nve_corpus = nve_articles['headline'].values\n",
    "sample_query = \"did bank employees come out wealthier from the recession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]\n",
    "\n",
    "\n",
    "class TfIdfer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)\n",
    "        self.tfidf_corpus = None\n",
    "        self.vocab = None\n",
    "        \n",
    "    def train(corpus):\n",
    "        self.tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "        self.vocab = self.tfidf_vectorizer.vocabulary_\n",
    "    \n",
    "    def transform(doc):\n",
    "        return self.tfidf_vectorizer.transform([doc])\n",
    "    \n",
    "    # def update():\n",
    "        # To do...\n",
    "        # self.tfidf_vectorizer = TfidfVectorizer(vocabulary=self.vocab,\n",
    "        #                                         stop_words='english',\n",
    "        #                                         tokenizer=TokenLemmatizer(),\n",
    "        #                                         max_df=0.5, min_df=0.01)\n",
    "\n",
    "\n",
    "def calculate_similarities(doc, corpus):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)\n",
    "    tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    doc_vector = tfidf_vectorizer.transform([doc])\n",
    "\n",
    "    return linear_kernel(doc_vector, tfidf_corpus).flatten()\n",
    "\n",
    "\n",
    "def retreive_results(similarities, corpus):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    argmax_index = np.argmax(similarities)\n",
    "    score = max(similarities)\n",
    "    return {\"index\": argmax_index,\n",
    "            \"item\": corpus[argmax_index],\n",
    "            \"score\": score}\n",
    "\n",
    "\n",
    "def query_articles(query_statement):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    processed_query_statement = query_statement  # will first have to process the query_statement, but here it's assumed that's been done\n",
    "    \n",
    "    pve_similarities = calculate_similarities(processed_query_statement, pve_articles['headline'].values)\n",
    "    nve_similarities = calculate_similarities(processed_query_statement, nve_articles['headline'].values)\n",
    "    \n",
    "    results[\"positive\"] = retreive_results(pve_similarities, pve_articles['headline'].values)\n",
    "    results[\"negative\"] = retreive_results(nve_similarities, nve_articles['headline'].values)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized testing below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129    fitzy planning for federer to play davis cup\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pve_articles['headline'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    aust olympian leaves beijing over assault\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nve_articles['headline'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': {'index': 129,\n",
       "  'item': 'fitzy planning for federer to play davis cup',\n",
       "  'score': 1.0},\n",
       " 'negative': {'index': 0,\n",
       "  'item': 'total chief executive killed as plane hits snow plough',\n",
       "  'score': 0.0}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_articles(\"federer davis cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': {'index': 61,\n",
       "  'item': 'australia to attend burma donors meeting',\n",
       "  'score': 1.0},\n",
       " 'negative': {'index': 52,\n",
       "  'item': 'australia could lose a generation of manufacturing skills',\n",
       "  'score': 1.0}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_articles(\"olympian from australia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
