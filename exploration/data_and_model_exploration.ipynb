{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA  # for compactness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's sentiment intensity analyzer is based on a rule-based model that implements the model described here  _Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max.columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = pd.read_csv('../data/abcnews_million_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One million documents presents an extremely large corpus, that for the sake of this demonstration will make training excessively long without a smarter way in which to build up the TF-IDF matrix. For the present context then we randomly sample 1,000 documents for the experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines = raw_headlines.sample(1000)\n",
    "raw_headlines.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_headlines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A semantic intensity analyzer is constructed using NLTK's SIA (a robust, but rule-based, lexicographic heuristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()  # initialize a nltk semantic intensity analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_headlines = []\n",
    "\n",
    "for headline in raw_headlines['headline'].values:\n",
    "    sia_scores = sia.polarity_scores(headline)\n",
    "    sia_scores['headline'] = headline\n",
    "    scored_headlines.append(sia_scores)\n",
    "    \n",
    "headlines = pd.DataFrame(scored_headlines)\n",
    "# headlines['date'] = raw_headlines['date']  # in application the date is irrelevant and thus omitted here from further consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The memory footprint of this object is...\n",
    "sys.getsizeof(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly a single Mb, which is sufficiently small as to be maintainable in memory by the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compactness the set is divided into positively and negatively associated corpuses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pve_articles = headlines[headlines['compound'] >= 0.0]\n",
    "nve_articles = headlines[headlines['compound'] < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the articles\n",
    "print(\"{:.1f}% articles have positive sentiment\".format(len(pve_articles)/(len(pve_articles)+len(nve_articles))*100))\n",
    "print(\"{:.1f}% articles have negative sentiment\".format(len(nve_articles)/(len(pve_articles)+len(nve_articles))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total length is preserved\n",
    "len(pve_articles)+len(nve_articles)==len(raw_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct some regular expressions to be able to clean any text that is input into the system, and define a preprocessor method that lower cases the input text, \"cleans\" abbreviations, and removes general special characters, and strips dashes and underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_abbr = re.compile(r'(?:^|\\s)((?:\\w(\\.\\s|\\s|\\.))(?:\\w\\2)+)', re.UNICODE)\n",
    "re_abbr_separator = re.compile(r'(\\s|\\.)', re.UNICODE)\n",
    "re_numword = re.compile(r'(\\s\\d*\\s)|\\s\\d*\\.\\d*\\s', re.UNICODE)\n",
    "re_specialchar_removal = re.compile(r'(!|@|#|&|\\(|\\)|\\+|=|\\{|\\}|\\[|\\]|:|;|\\\"|\\'|,|\\.$|\\?)', re.UNICODE)\n",
    "re_specialchar_numsymbremoval = re.compile(r'(\\$|%)', re.UNICODE)\n",
    "re_dash_removal = re.compile(r'-|_', re.UNICODE)\n",
    "\n",
    "\n",
    "def abbreviations_to_words(text):\n",
    "    \"\"\"\n",
    "    Converts all abbreviations found in the input string to a single word format.\n",
    "    \"\"\"\n",
    "    text += \" \"\n",
    "    all_abbreviations = [x[0] for x in re_abbr.findall(text + \" \")]\n",
    "    for abbreviation in all_abbreviations:\n",
    "        new_form = re_abbr_separator.sub('', abbreviation)\n",
    "        text = text.replace(abbreviation, new_form)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"\n",
    "    Applies the following preprocessing steps to any input text:\n",
    "        - lowercases all text\n",
    "        - maps abbreviations to same format (e.g., A.D., A. D., A D to AD)\n",
    "        - removes general special characters (e.g., an '!' or an '&' symbol)\n",
    "        - splits words that contains dashes or underscores\n",
    "        - strips the any newline characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = abbreviations_to_words(text)\n",
    "    text = re_specialchar_removal.sub('', text)\n",
    "    text = re_dash_removal.sub(' ', text)\n",
    "    text = re_specialchar_numsymbremoval.sub('', text) # depending on intent, this should be optional\n",
    "    text = re_numword.sub(' numword ', text)           # and this one\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def score_text(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scored_text = []\n",
    "    sia_score = sia.polarity_scores(text)\n",
    "    sia_score['headline'] = text\n",
    "    scored_text.append(sia_score)\n",
    "    return pd.DataFrame(scored_text)\n",
    "\n",
    "\n",
    "def add_article(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    global pve_articles\n",
    "    global nve_articles\n",
    "    text = preprocessor(text)\n",
    "    scored_text = score_text(text)\n",
    "    if scored_text['compound'].values[0] >= 0:\n",
    "        pve_articles = pd.concat([pve_articles, scored_text]).reset_index(drop=True)\n",
    "        print(\"\\\"{}\\\" added to the pve article set with score {}.\".format(scored_text['headline'].values[0], scored_text['compound'].values[0]))\n",
    "        return 1\n",
    "    else:\n",
    "        nve_articles = pd.concat([nve_articles, scored_text]).reset_index(drop=True)\n",
    "        print(\"\\\"{}\\\" added to the nve article set with score {}.\".format(scored_text['headline'].values[0], scored_text['compound'].values[0]))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Googled around for some headlines from CBC.ca/news, NPR.org, Bloomberg.com, and the MIT Technology Review (https://www.technologyreview.com/) to test the functionality of this preprocessing; the resulting examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_tests = [\"Hi! My name is Alexander.\",  # not actually a news item :P\n",
    "                  \"\\'Storm of a lifetime\\': 1.7 million ordered to flee approaching fury of Florence\",\n",
    "                  \"Trump Administration Transferred $9.8-Million From F.E.M.A. To I.C.E.\",\n",
    "                  \"A $100 Million Haircut for the Buyout Crowd\",\n",
    "                  \"Crypto Plunges 80%! Now Worse Than the Dot-Com Crash!\",\n",
    "                  \"How Bank Workers Emerged From the Crash $12.5 Billion Richer\",\n",
    "                  \"H.N.A.'s Debt Declines for First Time, Shrinking by $8.3-Billion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_processed_tests = []\n",
    "for hl in headline_tests:\n",
    "    headline_processed_tests.append(preprocessor(hl))\n",
    "\n",
    "headline_processed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the articles, performing the preprocessing as an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hl in headline_tests:\n",
    "    add_article(hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a new headline is introduced that is roughly equivalent to one of the above to act as a comparative reference in evaluting the efficacy of what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_headline = preprocessor(\"Big Bank Employees Came Out Wealthier from the Great Recession!\")\n",
    "new_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match different, but analogous, sentences the body text should be simplified. One way to accomplish this is to remove stopwords, stem, and lemmatize all the sentences to be matched. Here, a tokenizer and lemmatizer are applied to the text. Text objects can then be compared by calculating and evaluating the cosine distance between the resultant document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initial attempt utilized Porter Stemming instead of Lemmatization\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  # porter stemming, rule-based word reduction\n",
    "    tokens = nltk.word_tokenize(text)  # generate word tokens\n",
    "    return [i for i in [stemmer.stem(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above method is recomposed as a class that can be passed directly to the TF-IDF vectorizer that follows;\n",
    "# lemmatization is selected and applied in place of the Porter stemming\n",
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection of the output from the TokenLemmatizer() on the `new_headline`\n",
    "tokenize_and_stem(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is used to construct the vectorized forms of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance while varying `ngram_range` between one and three resulted in significantly decreased performance. This is likely due to the fact that we're specifically analyzing analogue sentences, where not only are the words between two documents potentially in a different order, but synonyms for words may effect the groupings as well. In such a case the construction of n-grams will artificially increase the distance between associated word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of `max_df` and `min_df` below made intuitively with several manual iterations used to optimize performance (default values are [1.0, 1.0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)  # hence, no n-grams here\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(headline_processed_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's take a look at what the train_tfidf matrix looks like, as a dataframe\n",
    "pd.DataFrame(train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting a new sentence against this corpus, the sentence's similarity is evaluated as the cosine similarity between the new sentence and the existing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tfidf_vectorizer.transform([new_headline])\n",
    "print(response)  # CSR result, feature(s) enumerated and significance, matching entirely due to the presence of the word 'bank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our matricies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = linear_kernel(response, train_tfidf).flatten()\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive a tuple of the maximally similar result\n",
    "np.argmax(similarity), max(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we rearrange this code for future implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and reprint our corpus and reframe the `new_headline` in the form of a query\n",
    "pve_corpus = pve_articles['headline'].values\n",
    "nve_corpus = nve_articles['headline'].values\n",
    "sample_query = \"did bank employees come out wealthier from the recession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLemmatizer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [i for i in [self.lemmatizer.lemmatize(t) for t in tokens] if len(i) > 2]\n",
    "\n",
    "\n",
    "class TfIdfer(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)\n",
    "        self.tfidf_corpus = None\n",
    "        self.vocab = None\n",
    "        \n",
    "    def train(corpus):\n",
    "        self.tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "        self.vocab = self.tfidf_vectorizer.vocabulary_\n",
    "    \n",
    "    def transform(doc):\n",
    "        return self.tfidf_vectorizer.transform([doc])\n",
    "    \n",
    "    # def update():\n",
    "        # To do...\n",
    "        # self.tfidf_vectorizer = TfidfVectorizer(vocabulary=self.vocab,\n",
    "        #                                         stop_words='english',\n",
    "        #                                         tokenizer=TokenLemmatizer(),\n",
    "        #                                         max_df=0.5, min_df=0.01)\n",
    "\n",
    "\n",
    "def calculate_similarities(doc, corpus):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=TokenLemmatizer(), max_df=0.5, min_df=0.01)\n",
    "    tfidf_corpus = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    doc_vector = tfidf_vectorizer.transform([doc])\n",
    "\n",
    "    return linear_kernel(doc_vector, tfidf_corpus).flatten()\n",
    "\n",
    "\n",
    "def retreive_results(similarities, corpus):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    argmax_index = np.argmax(similarities)\n",
    "    score = max(similarities)\n",
    "    return {\"index\": argmax_index,\n",
    "            \"item\": corpus[argmax_index],\n",
    "            \"score\": score}\n",
    "\n",
    "\n",
    "def query_articles(query_statement):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    processed_query_statement = query_statement  # will first have to process the query_statement, but here it's assumed that's been done\n",
    "    \n",
    "    pve_similarities = calculate_similarities(processed_query_statement, pve_articles['headline'].values)\n",
    "    nve_similarities = calculate_similarities(processed_query_statement, nve_articles['headline'].values)\n",
    "    \n",
    "    results[\"positive\"] = retreive_results(pve_similarities, pve_articles['headline'].values)\n",
    "    results[\"negative\"] = retreive_results(nve_similarities, nve_articles['headline'].values)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized testing below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pve_articles['headline'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nve_articles['headline'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_articles(\"federer davis cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_articles(\"olympian from australia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
